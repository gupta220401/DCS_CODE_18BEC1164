{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a737ccec-50d4-44f9-b9ff-be059eb7d1bf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "question 1 - (570. Managers with at Least 5 Direct Reports)"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+---------+\n| id| name|department|managerId|\n+---+-----+----------+---------+\n|101| John|         A|     null|\n|102|  Dan|         A|      101|\n|103|James|         A|      101|\n|104|  Amy|         A|      101|\n|105| Anne|         A|      101|\n|106|  Ron|         B|      101|\n+---+-----+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "dataa = [\n",
    "    (101, 'John', 'A', None),   # null for managerId\n",
    "    (102, 'Dan', 'A', 101),\n",
    "    (103, 'James', 'A', 101),\n",
    "    (104, 'Amy', 'A', 101),\n",
    "    (105, 'Anne', 'A', 101),\n",
    "    (106, 'Ron', 'B', 101)\n",
    "]\n",
    "\n",
    "schemaa= StructType([StructField('id',IntegerType(),False),StructField('name',StringType(),False),StructField('department',StringType(),False),StructField('managerId',IntegerType(),True)])\n",
    "\n",
    "df=spark.createDataFrame(dataa,schemaa)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "373ec4bf-3534-4b4e-baa4-7d6c5b22981c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "question 1 - (570. Managers with at Least 5 Direct Reports)"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th></tr></thead><tbody><tr><td>John</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "John"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "variablee=df.groupBy('managerId').agg(count('managerId').alias('countt')).filter((col('countt')>=5)&(col('managerId').isNotNull())).select(col('managerId')).collect()\n",
    "manager_ids=[ss['managerId'] for ss in variablee]\n",
    "    \n",
    "df.select(col('name')).filter(col('id').isin(manager_ids)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b3f8bac-d157-41b4-b576-e193e6e3ef02",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1757. Recyclable and Low Fat Products"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+\n|product_id|low_fats|recyclable|\n+----------+--------+----------+\n|         0|       Y|         N|\n|         1|       Y|         Y|\n|         2|       N|         Y|\n|         3|       Y|         Y|\n|         4|       N|         N|\n+----------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "dataa = [\n",
    "    (0, 'Y', 'N'),\n",
    "    (1, 'Y', 'Y'),\n",
    "    (2, 'N', 'Y'),\n",
    "    (3, 'Y', 'Y'),\n",
    "    (4, 'N', 'N')\n",
    "]\n",
    "\n",
    "schemaa = StructType([\n",
    "    StructField('product_id', IntegerType(), False),\n",
    "    StructField('low_fats', StringType(), False),\n",
    "    StructField('recyclable', StringType(), False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(dataa, schemaa)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a47aee1-e89a-46a6-bcd9-e6fd1146f93b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1757. Recyclable and Low Fat Products"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th></tr></thead><tbody><tr><td>1</td></tr><tr><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1
        ],
        [
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.filter((col('low_fats')=='Y')&(col('recyclable')=='Y')).select(col('product_id')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dbaa812-5812-4961-8d1f-2b936f9c6039",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "584. Find Customer Referee"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n| id|name|referee_id|\n+---+----+----------+\n|  1|Will|      null|\n|  2|Jane|      null|\n|  3|Alex|         2|\n|  4|Bill|      null|\n|  5|Zack|         1|\n|  6|Mark|         2|\n+---+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "dataa = [\n",
    "    (1, 'Will', None),\n",
    "    (2, 'Jane', None),\n",
    "    (3, 'Alex', 2),\n",
    "    (4, 'Bill', None),\n",
    "    (5, 'Zack', 1),\n",
    "    (6, 'Mark', 2)\n",
    "]\n",
    "\n",
    "schemaa = StructType([\n",
    "    StructField('id', IntegerType(), False),\n",
    "    StructField('name', StringType(), False),\n",
    "    StructField('referee_id', IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(dataa, schemaa)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d06c5169-26ef-45f9-8294-2947c01a641f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "584. Find Customer Referee"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th></tr></thead><tbody><tr><td>Will</td></tr><tr><td>Jane</td></tr><tr><td>Bill</td></tr><tr><td>Zack</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Will"
        ],
        [
         "Jane"
        ],
        [
         "Bill"
        ],
        [
         "Zack"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.filter((col('referee_id').isNull()) | (col('referee_id')!=2)).select(col('name')).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc7c4126-1bcb-47e2-bbb4-756460790a83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\uD83D\uDCCC datetime and datetime.date — explained simply:\n",
    "In Python, there is a built-in module called datetime that lets you work with dates and times.\n",
    "\n",
    "\uD83E\uDDF1 datetime module:\n",
    "It contains many classes — two important ones are:\n",
    "\n",
    "datetime.date — only stores a date (like 2024-08-12)\n",
    "\n",
    "datetime.datetime — stores a date and time (like 2024-08-12 14:30:00)\n",
    "\n",
    "✅ datetime.date\n",
    "This class represents a date (year, month, day) without time.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import datetime\n",
    "\n",
    "d = datetime.date(2024, 8, 12)\n",
    "print(d)   # Output: 2024-08-12\n",
    "So in your PySpark schema, when you declare a column as DateType(), it's best to use values of type datetime.date.\n",
    "\n",
    "\uD83E\uDDE0 Simple analogy:\n",
    "Concept\tExample\n",
    "String date\t'2024-08-12'\n",
    "datetime.date\tdatetime.date(2024, 8, 12)\n",
    "datetime.datetime\tdatetime.datetime(2024, 8, 12, 14, 30)\n",
    "\n",
    "\uD83D\uDCCC Summary:\n",
    "datetime is the module\n",
    "\n",
    "datetime.date is the class to create date-only objects\n",
    "\n",
    "It's the safest format to use for DateType() in PySpark\n",
    "\n",
    "Let me know if you want a side-by-side code example using all three!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "102b9a91-5eb6-4b52-933c-cd1d9ad35746",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1148. Article Views I"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+----------+\n|article_id|author_id|viewer_id| view_date|\n+----------+---------+---------+----------+\n|         1|        3|        5|2019-08-01|\n|         1|        3|        6|2019-08-02|\n|         2|        7|        7|2019-08-01|\n|         2|        7|        6|2019-08-02|\n|         4|        7|        1|2019-07-22|\n|         3|        4|        4|2019-07-21|\n|         3|        4|        4|2019-07-21|\n+----------+---------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "\n",
    "dataa = [\n",
    "    (1, 3, 5, datetime.date(2019, 8, 1)),\n",
    "    (1, 3, 6, datetime.date(2019, 8, 2)),\n",
    "    (2, 7, 7, datetime.date(2019, 8, 1)),\n",
    "    (2, 7, 6, datetime.date(2019, 8, 2)),\n",
    "    (4, 7, 1, datetime.date(2019, 7, 22)),\n",
    "    (3, 4, 4, datetime.date(2019, 7, 21)),\n",
    "    (3, 4, 4, datetime.date(2019, 7, 21))\n",
    "]\n",
    "\n",
    "schemaa = StructType([\n",
    "    StructField('article_id', IntegerType(), False),\n",
    "    StructField('author_id', IntegerType(), False),\n",
    "    StructField('viewer_id', IntegerType(), False),\n",
    "    StructField('view_date', DateType(), False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(dataa, schemaa)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f60a6756-0f25-4479-bdff-43eda85da391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>author_id</th></tr></thead><tbody><tr><td>4</td></tr><tr><td>7</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4
        ],
        [
         7
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "author_id",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df.filter(col('author_id')==col('viewer_id')).select(col('author_id')).distinct().sort(col('author_id').asc()).display()\n",
    "df.filter(col('author_id')==col('viewer_id')).select(col('author_id')).dropDuplicates(subset=['author_id']).sort(col('author_id').asc()).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdf5dd09-df37-4412-b91f-147d91d21237",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1683. Invalid Tweets"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------------+\n|tweet_id|content                          |\n+--------+---------------------------------+\n|1       |Let us Code                      |\n|2       |More than fifteen chars are here!|\n+--------+---------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "dataa = [\n",
    "    (1, \"Let us Code\"),\n",
    "    (2, \"More than fifteen chars are here!\")\n",
    "]\n",
    "\n",
    "schemaa = StructType([\n",
    "    StructField('tweet_id', IntegerType(), False),\n",
    "    StructField('content', StringType(), False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(dataa, schemaa)\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "471cf00a-0d70-4d35-bad7-63f63378a9c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1683. Invalid Tweets"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>tweet_id</th></tr></thead><tbody><tr><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "tweet_id",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.filter(length(col('content'))>15).select('tweet_id').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f7f7726-d229-4974-b7ec-967b49136f04",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1378. Replace Employee ID With The Unique Identifier"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| id|    name|\n+---+--------+\n|  1|   Alice|\n|  7|     Bob|\n| 11|    Meir|\n| 90| Winston|\n|  3|Jonathan|\n+---+--------+\n\n+---+---------+\n| id|unique_id|\n+---+---------+\n|  3|        1|\n| 11|        2|\n| 90|        3|\n+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Employees table\n",
    "dataa_employees = [\n",
    "    (1, \"Alice\"),\n",
    "    (7, \"Bob\"),\n",
    "    (11, \"Meir\"),\n",
    "    (90, \"Winston\"),\n",
    "    (3, \"Jonathan\")\n",
    "]\n",
    "\n",
    "schemaa_employees = StructType([\n",
    "    StructField('id', IntegerType(), False),\n",
    "    StructField('name', StringType(), False)\n",
    "])\n",
    "\n",
    "df_employees = spark.createDataFrame(dataa_employees, schemaa_employees)\n",
    "df_employees.show()\n",
    "\n",
    "# EmployeeUNI table\n",
    "dataa_employeeuni = [\n",
    "    (3, 1),\n",
    "    (11, 2),\n",
    "    (90, 3)\n",
    "]\n",
    "\n",
    "schemaa_employeeuni = StructType([\n",
    "    StructField('id', IntegerType(), False),\n",
    "    StructField('unique_id', IntegerType(), False)\n",
    "])\n",
    "\n",
    "df_employeeuni = spark.createDataFrame(dataa_employeeuni, schemaa_employeeuni)\n",
    "df_employeeuni.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a027659-c5cd-4412-8180-c20a73a96636",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755000586022}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "1378. Replace Employee ID With The Unique Identifier"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>unique_id</th><th>name</th></tr></thead><tbody><tr><td>null</td><td>Alice</td></tr><tr><td>null</td><td>Bob</td></tr><tr><td>2</td><td>Meir</td></tr><tr><td>3</td><td>Winston</td></tr><tr><td>1</td><td>Jonathan</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         "Alice"
        ],
        [
         null,
         "Bob"
        ],
        [
         2,
         "Meir"
        ],
        [
         3,
         "Winston"
        ],
        [
         1,
         "Jonathan"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "unique_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_employees.join(df_employeeuni,df_employees.id==df_employeeuni.id,'left').select(col('unique_id'),col('name')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec963e0d-1909-4db1-8b41-6bbfb0e8fd26",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1068. Product Sales Analysis I"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+--------+-----+\n|sale_id|product_id|year|quantity|price|\n+-------+----------+----+--------+-----+\n|      1|       100|2008|      10| 5000|\n|      2|       100|2009|      12| 5000|\n|      7|       200|2011|      15| 9000|\n+-------+----------+----+--------+-----+\n\n+----------+------------+\n|product_id|product_name|\n+----------+------------+\n|       100|       Nokia|\n|       200|       Apple|\n|       300|     Samsung|\n+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Sales table\n",
    "dataa_sales = [\n",
    "    (1, 100, 2008, 10, 5000),\n",
    "    (2, 100, 2009, 12, 5000),\n",
    "    (7, 200, 2011, 15, 9000)\n",
    "]\n",
    "\n",
    "schemaa_sales = StructType([\n",
    "    StructField('sale_id', IntegerType(), False),\n",
    "    StructField('product_id', IntegerType(), False),\n",
    "    StructField('year', IntegerType(), False),\n",
    "    StructField('quantity', IntegerType(), False),\n",
    "    StructField('price', IntegerType(), False)\n",
    "])\n",
    "\n",
    "df_sales = spark.createDataFrame(dataa_sales, schemaa_sales)\n",
    "df_sales.show()\n",
    "\n",
    "# Product table\n",
    "dataa_product = [\n",
    "    (100, \"Nokia\"),\n",
    "    (200, \"Apple\"),\n",
    "    (300, \"Samsung\")\n",
    "]\n",
    "\n",
    "schemaa_product = StructType([\n",
    "    StructField('product_id', IntegerType(), False),\n",
    "    StructField('product_name', StringType(), False)\n",
    "])\n",
    "\n",
    "df_product = spark.createDataFrame(dataa_product, schemaa_product)\n",
    "df_product.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c06e7902-f0c6-4cff-ba3b-668c5e096d83",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1068. Product Sales Analysis I"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_name</th><th>year</th><th>price</th></tr></thead><tbody><tr><td>Nokia</td><td>2008</td><td>5000</td></tr><tr><td>Nokia</td><td>2009</td><td>5000</td></tr><tr><td>Apple</td><td>2011</td><td>9000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Nokia",
         2008,
         5000
        ],
        [
         "Nokia",
         2009,
         5000
        ],
        [
         "Apple",
         2011,
         9000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sales.join(df_product,df_product.product_id==df_sales.product_id,'left').select(col('product_name'),col('year'),col('price')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e6a40d-71b2-427c-a17a-82aa0186b403",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1581. Customer Who Visited but Did Not Make Any Transactions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n|visit_id|customer_id|\n+--------+-----------+\n|       1|         23|\n|       2|          9|\n|       4|         30|\n|       5|         54|\n|       6|         96|\n|       7|         54|\n|       8|         54|\n+--------+-----------+\n\n+--------------+--------+------+\n|transaction_id|visit_id|amount|\n+--------------+--------+------+\n|             2|       5|   310|\n|             3|       5|   300|\n|             9|       5|   200|\n|            12|       1|   910|\n|            13|       2|   970|\n+--------------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Visits table\n",
    "dataa_visits = [\n",
    "    (1, 23),\n",
    "    (2, 9),\n",
    "    (4, 30),\n",
    "    (5, 54),\n",
    "    (6, 96),\n",
    "    (7, 54),\n",
    "    (8, 54)\n",
    "]\n",
    "\n",
    "schemaa_visits = StructType([\n",
    "    StructField('visit_id', IntegerType(), False),\n",
    "    StructField('customer_id', IntegerType(), False)\n",
    "])\n",
    "\n",
    "df_visits = spark.createDataFrame(dataa_visits, schemaa_visits)\n",
    "df_visits.show()\n",
    "\n",
    "# Transactions table\n",
    "dataa_transactions = [\n",
    "    (2, 5, 310),\n",
    "    (3, 5, 300),\n",
    "    (9, 5, 200),\n",
    "    (12, 1, 910),\n",
    "    (13, 2, 970)\n",
    "]\n",
    "\n",
    "schemaa_transactions = StructType([\n",
    "    StructField('transaction_id', IntegerType(), False),\n",
    "    StructField('visit_id', IntegerType(), False),\n",
    "    StructField('amount', IntegerType(), False)\n",
    "])\n",
    "\n",
    "df_transactions = spark.createDataFrame(dataa_transactions, schemaa_transactions)\n",
    "df_transactions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a945c4-a462-4083-bd62-a8186399252b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1581. Customer Who Visited but Did Not Make Any Transactions"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>count_no_trans</th></tr></thead><tbody><tr><td>96</td><td>1</td></tr><tr><td>54</td><td>2</td></tr><tr><td>30</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         96,
         1
        ],
        [
         54,
         2
        ],
        [
         30,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "count_no_trans",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df_visits.join(df_transactions,df_visits.visit_id==df_transactions.visit_id,'left').filter(col('amount').isNull()).groupby(col('customer_id')).agg(count('*').alias('count_no_trans')).select(col('customer_id'),col('count_no_trans')).sort(col('customer_id').desc()).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a20b41c-14dd-47e8-9470-249805c99dbc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "197. Rising Temperature"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+\n| id|recordDate|temperature|\n+---+----------+-----------+\n|  1|2015-01-01|         10|\n|  2|2015-01-02|         25|\n|  3|2015-01-03|         20|\n|  4|2015-01-04|         30|\n+---+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "\n",
    "# Weather table data\n",
    "dataa_weather = [\n",
    "    (1, datetime.date(2015, 1, 1), 10),\n",
    "    (2, datetime.date(2015, 1, 2), 25),\n",
    "    (3, datetime.date(2015, 1, 3), 20),\n",
    "    (4, datetime.date(2015, 1, 4), 30)\n",
    "]\n",
    "\n",
    "# Schema for Weather table\n",
    "schemaa_weather = StructType([\n",
    "    StructField('id', IntegerType(), False),\n",
    "    StructField('recordDate', DateType(), False),\n",
    "    StructField('temperature', IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df_weather = spark.createDataFrame(dataa_weather, schemaa_weather)\n",
    "df_weather.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4c55107-6d0c-453a-90b6-c16519eba2a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###if we use self join, below steps are mandatory:\n",
    "1) df_weather1=df_weather.alias('a')\n",
    "2) df_weather2=df_weather.alias('b')\n",
    "\n",
    "and wherever we try to refer the columns in the two df's we should write like col('a.recordDate')==col('b.recordDate') insetad of df_weather1.recordDtae==df_weather2.recordDtae like we do in normal sql joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d57431b-e0ab-4126-bdc4-3443afd85555",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755012431334}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "197. Rising Temperature"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th></tr></thead><tbody><tr><td>2</td></tr><tr><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2
        ],
        [
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_weather1=df_weather.alias('a')\n",
    "df_weather2=df_weather.alias('b')\n",
    "df_weather1.join(df_weather2,col('b.recordDate')==date_add(col('a.recordDate'),1),'inner').filter(col('b.temperature')>col('a.temperature')).select(col('b.id')).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbc9bf36-9efe-48e7-aa66-87e2c7c7fa3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Follow these when using window functions\n",
    "\n",
    "1)these are mandatory:   \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *  ------------------>> this will have lead lag etc functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd333ba8-0d77-4f7a-a60f-50ebbcda3404",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755013090570}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "197. Rising Temperature"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th></tr></thead><tbody><tr><td>2</td></tr><tr><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2
        ],
        [
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "df_weather.select(when((lead(col('temperature')).over(Window.orderBy(col('recordDate')) )>col('temperature'))&(lead(col('recordDate')).over(Window.orderBy(col('recordDate')))==col('recordDate')+1),lead(col('id')).over(Window.orderBy(col('recordDate')))).alias('id')).filter(col('id').isNotNull()).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}